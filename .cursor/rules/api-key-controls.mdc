---
alwaysApply: true
---

# API Key Controls & Workflow Management System

## Overview

Enables OpenRouter integration with automatic LM Studio fallback, plus a dynamic workflow prediction panel showing the next 20 API calls with boost feature.

**Key Features:**
- **Per-Role OpenRouter Selection**: Each role independently uses LM Studio or OpenRouter
- **Global OpenRouter API Key**: Single key for all per-role OpenRouter selections (separate from boost)
- **LM Studio Fallback**: Optional fallback per role on credit exhaustion
- **Boost Mode**: Separate boost API key for selective task acceleration via three modes:
  - **Boost Next X Calls**: Counter-based, next X API calls regardless of task ID
  - **Category Boost**: Role-based, boosts all calls for specific role categories
  - **Per-task Toggle**: Task ID based (legacy)
- **System works without LM Studio**: Defaults to OpenRouter when LM Studio unavailable

---

## Architecture Components

### Boost and Parallel Execution

**Boost is a ROUTING decision, NOT a CONCURRENCY decision.**
- Boost affects which API endpoint is used, NOT whether submitters run in parallel or serial
- Aggregation submitters ALWAYS run in parallel regardless of boost status (unless single-model mode)
- Single-model mode: triggered when all submitters AND validator use the SAME configured model ID. Boost routing does NOT trigger single-model mode.

### Backend Core

#### OpenRouterClient (`backend/shared/openrouter_client.py`)
- Async HTTP client. Base URL: `https://openrouter.ai/api/v1`
- App Attribution Headers: `HTTP-Referer: https://intrafere.com/moto-autonomous-home-ai/`, `X-Title: MOTO Deep Research Harness`
- Credit exhaustion detection: HTTP 402 OR error messages containing "credit", "insufficient", "balance", "quota"
- Raises `CreditExhaustionError` on exhaustion (no retries). Retries transient errors (max 3).
- Temperature=0.0 default. Stop sequences on all requests: `\n}\n\n`, `\n]\n\n`, `\n}\n\n\n`, `\n]\n\n\n`

#### APIClientManager (`backend/shared/api_client_manager.py`)
- Central router for all API calls: boost check → role's OpenRouter (with permanent fallback) → LM Studio
- Tracks fallback state per role: `_role_fallback_state: Dict[str, str]`
- Lazy initialization: OpenRouter client initializes from `rag_config.openrouter_api_key` when first needed

**CRITICAL REQUIREMENT - Role Configuration:**
- **EVERY role calling `api_client_manager.generate_completion()` MUST be configured via `api_client_manager.configure_role()`**
- This includes: aggregator submitters/validator, compiler submitters/validator/critique, autonomous agents, Tier 3 final answer agents

**Boost Mode Priority** (`should_use_boost(task_id)`):
1. Boost Next X: `boost_next_count > 0` → True
2. Category Boost: `_extract_role_prefix(task_id) in boosted_categories` → True
3. Per-task Toggle: `task_id in boosted_task_ids` → True

**Counter Decrement:** `boost_next_count` decrements ONLY on successful boost API calls. Failed/exhausted calls do NOT decrement.

**Permanent Fallback:** Once a role falls back to LM Studio due to credit exhaustion, it NEVER retries OpenRouter for that session. Each role has independent fallback state. If no fallback configured: raises RuntimeError.

**Categories from role_id:**
- `aggregator_submitter_*` → "Aggregator Submitters"
- `aggregator_validator` → "Aggregator Validator"
- `compiler_high_context` → "Compiler High-Context"
- `compiler_high_param` → "Compiler High-Param"
- `compiler_validator` → "Compiler Validator"
- `autonomous_*` → "Autonomous"

#### BoostManager (`backend/shared/boost_manager.py`)
- Singleton. Key methods: `set_boost_config`, `clear_boost`, `set_boost_next_count`, `toggle_category_boost`, `toggle_task_boost`, `should_use_boost` (main check for coordinators), `consume_boost_count` (only after successful boost call), `is_task_boosted` (DO NOT USE in coordinators — legacy only)
- Boost uses a **separate** OpenRouter API key. Temporary `OpenRouterClient` created per boosted task, closed immediately after.

#### BoostLogger (`backend/shared/boost_logger.py`)
- Singleton. Log file: `backend/data/boost_api_log.txt`
- Methods: `log_api_call`, `get_logs(limit)`, `clear_logs`, `get_stats`

#### Workflow Task Generation (Direct from Agent Sequences)
Coordinators read actual agent `task_sequence` counters — no prediction. Task IDs exactly match what agents will generate.
- Aggregator: `agg_sub{N}_{seq:03d}`, `agg_val_{seq:03d}`
- Compiler: `comp_hc_{seq:03d}`, `comp_hp_{seq:03d}`, `comp_val_{seq:03d}`
- Autonomous: `auto_ts_{seq:03d}`, `auto_tv_{seq:03d}`

---

## Configuration Models

### ModelConfig
```python
class ModelConfig(BaseModel):
    provider: Literal["lm_studio", "openrouter"] = "lm_studio"
    model_id: str
    openrouter_model_id: Optional[str] = None
    openrouter_provider: Optional[str] = None  # e.g., "Anthropic"
    lm_studio_fallback_id: Optional[str] = None
    context_window: int = 131072
    max_output_tokens: int = 25000
```

### BoostConfig
```python
class BoostConfig(BaseModel):
    enabled: bool = False
    openrouter_api_key: str = ""
    boost_model_id: str = ""
    boost_provider: Optional[str] = None
    boost_context_window: int = 131072
    boost_max_output_tokens: int = 25000
```

### WorkflowTask
```python
class WorkflowTask(BaseModel):
    task_id: str          # e.g., "agg_sub1_001"
    sequence_number: int  # 1-20
    role: str             # "Submitter 1", "Validator", "High-Context", etc.
    mode: Optional[str] = None  # "Construction", "Rigor", "Review", etc.
    provider: str = "lm_studio"
    using_boost: bool = False
    completed: bool = False
    active: bool = False
```

---

## Coordinator Integration

All coordinators track workflow via: `workflow_tasks`, `completed_task_ids`, `current_task_sequence`, `current_task_id`.

Key methods: `refresh_workflow_predictions()`, `get_next_task()`, `mark_task_completed(task_id)`, `mark_task_started(task_id)`.

Predictions refresh: after initialization, each task completion, mode switches, phase transitions.

---

## WebSocket Events

**Workflow:** `workflow_updated` (tasks+mode), `task_started` (task_id), `task_completed` (task_id+sequence)

**Boost:** `boost_enabled` (model_id, provider, context_window, max_output_tokens), `boost_disabled`, `task_boost_toggled` (task_id, boosted), `boost_next_count_set` (count), `category_boost_toggled` (category, boosted), `boost_credits_exhausted` (task_id, message)

**Fallback:** `openrouter_fallback` (role_id, reason, message, fallback_model), `openrouter_fallback_failed` (role_id, reason, message)

**Rate Limit:** `openrouter_rate_limit` (model, role_id, retry_after, message)

**Privacy:** `openrouter_privacy_error` (error_type, model, role_id, message, solution_url)

---

## Frontend Components

- **WorkflowPanel** (`WorkflowPanel.jsx`): Collapsible right-side panel, next 20 tasks, boost controls (Next X input, category chips), per-task click to toggle boost
- **BoostControlModal** (`BoostControlModal.jsx`): Configure boost API key, model, provider, context/max tokens
- **BoostLogs** (`BoostLogs.jsx`): Boost API call logs tab with auto-refresh and stats
- **OpenRouterApiKeyModal** (`OpenRouterApiKeyModal.jsx`): Set/test/clear global OpenRouter API key

---

## API Endpoints

### Boost (`backend/api/routes/boost.py`)
- `POST /api/boost/enable` — Enable boost (BoostConfig body)
- `POST /api/boost/disable` — Disable boost, clear all modes
- `GET /api/boost/status` — Current config, counts, categories
- `POST /api/boost/set-next-count` — Set Boost Next X counter `{ "count": int }`
- `POST /api/boost/toggle-category/{category}` — Toggle category boost
- `GET /api/boost/categories?mode=` — Available categories for mode
- `POST /api/boost/toggle-task/{task_id}` — Legacy per-task boost toggle
- `GET /api/boost/openrouter-models` — Fetch OpenRouter models (Bearer key header)
- `GET /api/boost/model-providers?model_id=` — Providers for a model
- `GET /api/boost/logs?limit=` — Recent boost logs
- `POST /api/boost/logs/clear` — Clear logs

### OpenRouter (`backend/api/routes/openrouter.py`)
- `GET /api/openrouter/lm-studio-availability` — LM Studio availability check
- `POST /api/openrouter/set-api-key` — Set and validate global OpenRouter key
- `DELETE /api/openrouter/api-key` — Clear key
- `GET /api/openrouter/api-key-status` — `{ has_key, enabled }`
- `GET /api/openrouter/models` — Available models
- `GET /api/openrouter/providers/{model_id}` — Providers for model
- `POST /api/openrouter/test-connection` — Test key without storing
- `GET /api/model-cache` — Cached model ID mapping (display_name → api_id)

### Workflow (`backend/api/routes/workflow.py`)
- `GET /api/workflow/predictions` — Next 20 predicted tasks
- `GET /api/workflow/history?limit=` — Completed tasks

---

## Error Handling

**Credit Exhaustion:** HTTP 402 or keywords "credit"/"insufficient"/"balance"/"quota" → `CreditExhaustionError` → permanent LM Studio fallback for that role (or RuntimeError if no fallback).

**Boost Exhaustion:** Falls back to primary for that task; boost stays enabled; counter NOT decremented.

**Rate Limits (Free Models):** HTTP 429 or "rate limit" keywords → 1-hour pause per `:free` model. Temporary fallback to LM Studio if configured. Methods: `_is_free_model()`, `_is_model_rate_limited()`, `_record_rate_limit()` on `OpenRouterClient`.

**Privacy Policy Errors:** HTTP 404 + "data policy" message → `OpenRouterPrivacyPolicyError`. Role-based: falls back to LM Studio if configured; Boost: raises RuntimeError. Fix: https://openrouter.ai/settings/privacy → enable data sharing.

---

## Configuration Persistence

**localStorage:** `openrouter_api_key`, `workflow_panel_collapsed`, `aggregatorConfig`, `compiler_settings`, `autonomousConfig`

**Session (in-memory):** fallback state per role, boosted task IDs, boost next count, boosted categories, completed task IDs. Boost logs persist to file (`boost_api_log.txt`).
